import pandas as pd
import numpy as np
from sklearn.metrics import recall_score, precision_score, confusion_matrix, accuracy_score, log_loss, auc, \
    matthews_corrcoef, f1_score, fbeta_score, roc_auc_score
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import ParameterGrid
import keras
from keras.layers import Dense, Embedding, Reshape, Add, Concatenate, Input, Dropout
from keras.models import Sequential, Model
from keras.utils import to_categorical
from keras.callbacks import EarlyStopping
from keras.wrappers.scikit_learn import KerasClassifier

def data_split(df, y_col, to_drop=[], random_state=None, hold1_size=.1, hold2_size=.1, hold3_size=.1):
    """ Splits the dataframe into the train set and 3 hold-out sets.

    Drops columns to drop and the target variable from the DataFrame df. Then the rows are reshuffled and split into 4 groups: train set and 3 hold-out sets.

    Args:
        df: pandas DataFrame with the data.
        y_col: the name of the column with the target variable.
        to_drop: list of columns to drop from further analysis.
        random_state: the seed used by the random number generator.
        hold1_size: the size of the first hold-out set as a fraction of total.
        hold2_size: the size of the second hold-out set as a fraction of total.
        hold3_size: the size of the third hold-out set as a fraction of total.

    Returns:
        A tuple of length 9 containing train set, 3 hold-out sets split of inputs and a list of column labels.
    """
    df_filtered = df.drop(columns=to_drop)
    rows = list(df_filtered.index)
    if random_state is not None:
        random.seed(random_state)
    random.shuffle(rows)
    length = len(rows)
    train_rows = rows[:int(length * (1 - (hold1_size + hold2_size + hold3_size)))]
    hold1_rows = rows[int(length * (1 - (hold1_size + hold2_size + hold3_size))):int(
        length * (1 - (hold2_size + hold3_size)))]
    hold2_rows = rows[int(length * (1 - (hold1_size + hold3_size))):int(length * (1 - hold3_size))]
    hold3_rows = rows[int(length * (1 - hold3_size)):]
    X_train = df_filtered.drop(columns=[y_col]).iloc[train_rows].values
    y_train = df_filtered.loc[train_rows, y_col].values
    X_hold1 = df_filtered.drop(columns=[y_col]).iloc[hold1_rows].values
    y_hold1 = df_filtered.loc[hold1_rows, y_col].values
    X_hold2 = df_filtered.drop(columns=[y_col]).iloc[hold2_rows].values
    y_hold2 = df_filtered.loc[hold2_rows, y_col].values
    X_hold3 = df_filtered.drop(columns=[y_col]).iloc[hold3_rows].values
    y_hold3 = df_filtered.loc[hold3_rows, y_col].values
    cols = df_filtered.drop(columns=[y_col]).columns
    return X_train, y_train, X_hold1, y_hold1, X_hold2, y_hold2, X_hold3, y_hold3, cols


def test_model(X_train, y_train, X_valid, y_valid, clf, oob_score=False, test='Validation'):
    """ Performes a model diagnostics with the training and validation sets.

    Makes predictions using the provided model on the training set and validation sets. Plots confusion matrices for each set.
    Returns the following diagnostic scores: precision score, recall score, F1 score, Fbeta score with beta=0.2, Mattews correlation coefficient,
    accuracy score, out-of-bag score.

    Args:
        X_train, y_train, X_valid, y_valid: trainig and validation sets.
        clf: classification model to be diagnosed.
        oob_score: boolean, indicating whether to include the out-of-bag scores calculated on the training data into the report.

    Returns:
        A pandas DataFrame with the diagnostic scores performed on the data sets.
    """
    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))
    sns.heatmap(confusion_matrix(y_train, clf.predict(X_train)), cmap=plt.cm.Blues, square=True, annot=True, fmt='d',
                vmin=0, ax=ax1);
    ax1.set_title('Training data')
    sns.heatmap(confusion_matrix(y_valid, clf.predict(X_valid)), cmap=plt.cm.Blues, square=True, annot=True, fmt='d',
                vmin=0, ax=ax2);
    ax2.set_title('{} data'.format(test));
    plt.tight_layout();
    y_train_pred = clf.predict(X_train)
    y_valid_pred = clf.predict(X_valid)
    y_train_pred_prob = clf.predict_proba(X_train)[:,1]
    y_valid_pred_prob = clf.predict_proba(X_valid)[:,1]
    score_table = pd.DataFrame(columns=['training data', 'validation data'])
    score_table.loc['roc_auc_score', 'training data'] = roc_auc_score(y_train, y_train_pred_prob)
    score_table.loc['accuracy score', 'training data'] = accuracy_score(y_train, y_train_pred)
    score_table.loc['roc_auc_score', 'validation data'] = roc_auc_score(y_valid, y_valid_pred_prob)
    score_table.loc['accuracy score', 'validation data'] = accuracy_score(y_valid, y_valid_pred)
    if oob_score:
        score_table.loc['out of bag score', 'training data'] = clf.oob_score_
    return score_table

# Function to create model, required for KerasClassifier
def create_model(neurons1=100, neurons2=50, dropout1=.8, dropout2=.8, n_cols=95):
    # create model
    model = Sequential()
    model.add(Dense(neurons1, activation='relu', input_shape=(n_cols, )))
    model.add(Dense(neurons2, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    # Compile model
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model


def preproc(X_train, X_val, cat_cols, cols_to_keep):
    """ Preprocesses training and validation sets ready for the neural network training.

    For each categorical column, the function remaps the values to the integer values and adds a one-dimensional
    numpy array of with mapped values to the output list. Other columns are treated as numeric and added as numpy array
    as the last element of the output list.

    :param X_train: training set, numpy array.
    :param X_val: validation set numpy array.
    :param cat_cols: names of the categorical columns
    :param cols_to_keep: names of columns selected for the model training.
    :return: a list with preprocessed data ready for an entity embedding neural network model.
    """
    other_cols = [not c in cat_cols for c in cols_to_keep]
    input_list_train = []
    input_list_val = []

    # the cols to be embedded: rescaling to range [0, # values)
    for c in cat_cols:
        el_index = cols_to_keep.index(c)
        raw_vals = np.unique(np.concatenate((X_train, X_val), axis=0)[:, el_index])
        raw_vals.sort()
        val_map = {}
        for i in range(len(raw_vals)):
            val_map[raw_vals[i]] = i
        input_list_train.append(np.vectorize(val_map.get)(X_train[:, el_index]))
        input_list_val.append(np.vectorize(val_map.get)(X_val[:, el_index]))

    # the rest of the columns
    input_list_train.append(X_train[:, other_cols])
    input_list_val.append(X_val[:, other_cols])

    return input_list_train, input_list_val

def build_embedding_network(X_train, X_val, cat_cols, cols_to_keep, n_num=100, n=60, d=False, verbose=False):
    """ Builds a neural network model with entity embedding for categorical variables.

    The function builds the neural network, for which it creates entity embedding for each categorical feature specified
    in cat_cols argument. Numerical features are projected ot a dense layer. Entity embedding with the numerical
    features projected to the dense layer project to 2 more layers. The final layer is the output layer.

    :param X_train: training set before preprocessing.
    :param X_val: validation set before preprocessing. The training and validation sets are need to calculate the
    dimensions of the entity embeddings.
    :param cat_cols: list with the names of categorical columns.
    :param cols_to_keep: list with the names of the columns that are selected for the model.
    :param n_num: number of neurons in the layer receiving input from the numerical feature matrix.
    :param n: number of neurons in the two hidden layers prior to the output layer.
    :param d: dropout setting for the two hidden layers before the output layer.
    :param verbose: if True prints the output with the size of the models.
    :return: neural network model.
    """
    inputs = []
    embeddings = []
    # models = []
    for categorical_var in cat_cols:
        if verbose:
            print("------------------------------------------------------------------")
            print("for categorical column ", categorical_var)
        input_cat = Input(shape=(1,))
        el_index = cols_to_keep.index(categorical_var)
        no_of_unique_cat = np.unique(np.concatenate((X_train, X_val), axis=0)[:, el_index]).size
        if verbose:
            print("number of unique cat", no_of_unique_cat)
        embedding_size = min(np.ceil((no_of_unique_cat) / 2), 50)
        embedding_size = int(embedding_size)
        if verbose:
            print("embedding_size set as ", embedding_size)
        embedding = Embedding(no_of_unique_cat + 1, embedding_size, input_length=1)(input_cat)
        embedding = Reshape(target_shape=(embedding_size,))(embedding)
        inputs.append(input_cat)
        embeddings.append(embedding)

    other_cols = [not c in cat_cols for c in cols_to_keep]
    len_other_cols = len([c for c in other_cols if c])
    if verbose:
        print("------------------------------------------------------------------")
        print('Numeric columns')
        print('Number of columns', len_other_cols)
    input_numeric = Input(shape=(len_other_cols,))
    embedding_numeric = Dense(n_num)(input_numeric)
    inputs.append(input_numeric)
    embeddings.append(embedding_numeric)

    x = Concatenate()(embeddings)
    x = Dense(n, activation='relu')(x)
    if d:
        x = Dropout(d)(x)
    x = Dense(n, activation='relu')(x)
    if d:
        x = Dropout(d)(x)
    output = Dense(1, activation='sigmoid')(x)

    model = Model(inputs, output)

    model.compile(loss='binary_crossentropy', optimizer='adam')
    return model

def build_embedding_network_3(X_train, X_val, cat_cols, cols_to_keep, n_num=120, n1=150, n2=50, d=False, lr=0.001, verbose=False):
    """Builds a neural network model with entity embedding for categorical variables.

    The function builds the neural network, for which it creates entity embedding for each categorical feature specified
    in cat_cols argument. Numerical features are projected ot a dense layer. Entity embeddings are concatenated with
    layer numerical features are projected and project to a dense layer with n1 neurons. There are additional two layers
    with n2 neurons each before projecting to the output layer.

    :param X_train: training set before preprocessing.
    :param X_val: validation set before preprocessing. The training and validation sets are need to calculate the
    dimensions of the entity embeddings.
    :param cat_cols: list with the names of categorical columns.
    :param cols_to_keep: list with the names of the columns that are selected for the model.
    :param n_num: number of neurons in the layer receiving input from the numerical feature matrix.
    :param n1: number of neurons in the layer that takes input from concatenated layer of entity embeddings and
    feature.
    :param n2: number of neurons in 2 dense layers prior to the output layer.
    :param d: dropout setting for the hidden layers before the output layer.
    :param lr: learning rate of the model
    :param verbose: if True prints the output with the size of the models.
    :return: neural network model.
    """
    inputs = []
    embeddings = []
    for categorical_var in cat_cols:
        if verbose:
            print("------------------------------------------------------------------")
            print("for categorical column ", categorical_var)
        input_cat = Input(shape=(1,))
        el_index = cols_to_keep.index(categorical_var)
        no_of_unique_cat = np.unique(np.concatenate((X_train, X_val), axis=0)[:, el_index]).size
        if verbose:
            print("number of unique cat", no_of_unique_cat)
        embedding_size = min(np.ceil((no_of_unique_cat) / 2), 50)
        embedding_size = int(embedding_size)
        if verbose:
            print("embedding_size set as ", embedding_size)
        embedding = Embedding(no_of_unique_cat + 1, embedding_size, input_length=1)(input_cat)
        embedding = Reshape(target_shape=(embedding_size,))(embedding)
        inputs.append(input_cat)
        embeddings.append(embedding)

    other_cols = [not c in cat_cols for c in cols_to_keep]
    len_other_cols = len([c for c in other_cols if c])
    if verbose:
        print("------------------------------------------------------------------")
        print('Numeric columns')
        print('Number of columns', len_other_cols)
    input_numeric = Input(shape=(len_other_cols,))
    embedding_numeric = Dense(n_num)(input_numeric)
    inputs.append(input_numeric)
    embeddings.append(embedding_numeric)

    x = Concatenate()(embeddings)
    x = Dense(n1, activation='relu')(x)
    if d:
        x = Dropout(d)(x)
    x = Dense(n1, activation='relu')(x)
    if d:
        x = Dropout(d)(x)
    x = Dense(n2, activation='relu')(x)
    if d:
        x = Dropout(d)(x)
    output = Dense(1, activation='sigmoid')(x)

    model = Model(inputs, output)

    optimizer = keras.optimizers.Adam(lr=lr)

    model.compile(loss='binary_crossentropy', optimizer=optimizer)
    return model

def grid_search(estimator, param_grid, X_train, y_train, X_test, y_test, batch_size=10000, nn=False):
    """ Performs grid search over parameter grid.

    Function iterates over the combinations of parameters in the parameter grid. Trains the estimator on the X_train.
    Evaluation is done on both the training set and validation set. The parameters reported are ROC AUC score calculated
    on the training and test sets.

    :param estimator: classifier for training
    :param param_grid: dictionary with parameters for the estimator
    :param X_train: training set input
    :param y_train: training set output
    :param X_test: test set input
    :param y_test: test set outpu
    :param batch_size: batch size for the neural network classifier
    :param nn: True if the estimator is a neural network model
    :return: a pandas DataFrame with the parameter combinations and the corresponding model evaluation metrics.
    """
    out = pd.DataFrame()
    for g in ParameterGrid(param_grid):
        estimator.set_params(**g)
        if nn:
            print('Fitting with params:', g)
            # Define early_stopping_monitor
            early_stopping_monitor = EarlyStopping(patience=2)

            # Fit the model
            estimator.fit(
                X_train,
                y_train,
                verbose=True,
                validation_split=0.3,
                epochs=1000,
                batch_size=batch_size,
                callbacks=[early_stopping_monitor])
            y_train_pred_prob_true = estimator.predict_proba(X_train)[:, 1]
            y_test_pred_prob_true = estimator.predict_proba(X_test)[:, 1]
            scores = {
                'train_roc_auc': roc_auc_score(y_train, y_train_pred_prob_true),
                'test_roc_auc': roc_auc_score(y_test, y_test_pred_prob_true)
            }
            print('Scores after fitting:', scores)
        else:
            estimator.fit(X_train, y_train)
            y_train_pred = estimator.predict(X_train)
            y_train_pred_prob = estimator.predict_proba(X_train)[:,1]
            y_test_pred = estimator.predict(X_test)
            y_test_pred_prob = estimator.predict_proba(X_test)[:,1]
            scores = {'train_accuracy': accuracy_score(y_train, y_train_pred),
                     'test_accuracy': accuracy_score(y_test, y_test_pred),
                     'train_roc_auc': roc_auc_score(y_train, y_train_pred_prob),
                     'test_roc_auc': roc_auc_score(y_test, y_test_pred_prob)}
        out_dict = g.copy()
        out_dict.update(scores)
        print(out_dict)
        out = out.append(out_dict, ignore_index=True)
    return out


def fit_GBC(alg, X, y, X_valid, y_valid, X_cols, printFeatureImportance=True):
    """ Fits the Gradient Boosting on the training set, evaluates the ROC AUC score on the training and validation set.
    Plots the feature importance plot.

    :param alg: Gradient Boosting Classifier model
    :param X: training set input variables
    :param y: training set target variable
    :param X_valid: validation set input variables
    :param y_valid: validation set target variable
    :param X_cols: column names
    :param printFeatureImportance: True if showing the feature importance plot
    """
    # Fit the algorithm on the data
    alg.fit(X, y)

    # Predict training set:
    y_predictions = alg.predict(X)
    y_predprob = alg.predict_proba(X)[:, 1]
    y_valid_predprob = alg.predict_proba(X_valid)[:, 1]

    # Print model report:
    print("\nModel Report")
    print("Accuracy : %.4g" % accuracy_score(y, y_predictions))
    print("AUC Score (Train): %f" % roc_auc_score(y, y_predprob))
    print("AUC Validation Score : %f" % roc_auc_score(y_valid, y_valid_predprob))

    # Print Feature Importance:
    if printFeatureImportance:
        feat_imp = pd.Series(alg.feature_importances_, X_cols).sort_values(ascending=False)
        feat_imp[:20].plot(kind='bar', title='Feature Importances', figsize=(12, 8))
        plt.ylabel('Feature Importance Score')